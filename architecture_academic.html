<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Deep Reinforcement Learning Architecture Analysis: V3 to SOTA on Humanoid-v4</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&family=Source+Code+Pro:wght@400;600&family=Lato:wght@300;400;700&display=swap');

:root {
  --ink:        #1a1a1a;
  --ink-mid:    #444444;
  --ink-light:  #777777;
  --ink-faint:  #aaaaaa;
  --paper:      #fafaf7;
  --paper-off:  #f2f1ec;
  --paper-rule: #e4e2d8;
  --rule:       #c8c4b4;
  --accent:     #2a4a8a;
  --accent-mid: #5070b0;
  --accent-lt:  #dce4f0;
  --red:        #8a2020;
  --red-lt:     #f5e8e8;
  --amber:      #7a5000;
  --amber-lt:   #faf0d8;
  --green:      #1a5030;
  --green-lt:   #e4f0e8;
  --purple:     #4a1a7a;
  --purple-lt:  #ede4f5;
  --col-w:      740px;
  --wide-w:     1100px;
}

*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

html { scroll-behavior: smooth; }

body {
  background: var(--paper);
  color: var(--ink);
  font-family: 'Lato', sans-serif;
  font-weight: 300;
  font-size: 15px;
  line-height: 1.75;
  -webkit-font-smoothing: antialiased;
}

/* ── LAYOUT ── */
.page { max-width: var(--wide-w); margin: 0 auto; padding: 0 32px 120px; }

/* ── HEADER ── */
.doc-header {
  border-bottom: 2px solid var(--ink);
  padding: 56px 0 36px;
  margin-bottom: 52px;
}
.doc-header .kicker {
  font-family: 'Source Code Pro', monospace;
  font-size: 0.72rem;
  font-weight: 600;
  letter-spacing: 0.18em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 14px;
}
.doc-header h1 {
  font-family: 'Libre Baskerville', serif;
  font-size: clamp(1.6rem, 3.5vw, 2.6rem);
  font-weight: 700;
  line-height: 1.2;
  color: var(--ink);
  max-width: 820px;
  margin-bottom: 20px;
}
.doc-header .abstract {
  max-width: 720px;
  color: var(--ink-mid);
  font-size: 0.95rem;
  line-height: 1.8;
  font-style: italic;
  border-left: 3px solid var(--rule);
  padding-left: 20px;
  margin-bottom: 24px;
}
.doc-meta {
  display: flex;
  gap: 32px;
  flex-wrap: wrap;
  font-size: 0.8rem;
  color: var(--ink-light);
  font-family: 'Source Code Pro', monospace;
}
.doc-meta span { color: var(--ink-mid); }

/* ── NAV ── */
.toc {
  float: right;
  width: 220px;
  margin: 0 0 32px 40px;
  background: var(--paper-off);
  border: 1px solid var(--rule);
  padding: 20px 22px;
}
.toc h4 {
  font-family: 'Source Code Pro', monospace;
  font-size: 0.7rem;
  text-transform: uppercase;
  letter-spacing: 0.15em;
  color: var(--ink-light);
  margin-bottom: 12px;
}
.toc ol { padding-left: 16px; }
.toc li { margin-bottom: 6px; }
.toc a {
  color: var(--accent);
  text-decoration: none;
  font-size: 0.82rem;
  line-height: 1.4;
}
.toc a:hover { text-decoration: underline; }

/* ── SECTIONS ── */
section { margin-bottom: 60px; }
section h2 {
  font-family: 'Libre Baskerville', serif;
  font-size: 1.35rem;
  font-weight: 700;
  color: var(--ink);
  margin-bottom: 6px;
  padding-bottom: 8px;
  border-bottom: 1px solid var(--rule);
}
.section-num {
  font-family: 'Source Code Pro', monospace;
  font-size: 0.75rem;
  color: var(--ink-faint);
  display: block;
  margin-bottom: 4px;
  letter-spacing: 0.1em;
}
section h3 {
  font-family: 'Libre Baskerville', serif;
  font-size: 1.08rem;
  font-weight: 700;
  margin: 28px 0 10px;
  color: var(--ink);
}
section h4 {
  font-size: 0.88rem;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 0.1em;
  color: var(--ink-mid);
  margin: 20px 0 8px;
  font-family: 'Source Code Pro', monospace;
}
p { color: var(--ink-mid); margin-bottom: 14px; font-size: 0.94rem; }
strong { color: var(--ink); font-weight: 700; }

/* ── CALLOUT BOXES ── */
.callout {
  border-left: 4px solid;
  padding: 14px 18px;
  margin: 20px 0;
  font-size: 0.88rem;
  line-height: 1.65;
}
.callout p { margin: 0; font-size: 0.88rem; }
.callout.error  { border-color: var(--red);   background: var(--red-lt);   color: #5a1010; }
.callout.warn   { border-color: var(--amber);  background: var(--amber-lt); color: #4a3000; }
.callout.note   { border-color: var(--accent); background: var(--accent-lt);color: #1a2a5a; }
.callout.good   { border-color: var(--green);  background: var(--green-lt); color: #0a2a18; }
.callout.paper  { border-color: var(--purple); background: var(--purple-lt);color: #2a0a4a; }
.callout-label {
  font-family: 'Source Code Pro', monospace;
  font-size: 0.68rem;
  font-weight: 600;
  letter-spacing: 0.12em;
  text-transform: uppercase;
  margin-bottom: 6px;
}

/* ── CODE ── */
code {
  font-family: 'Source Code Pro', monospace;
  font-size: 0.82em;
  background: var(--paper-off);
  border: 1px solid var(--rule);
  padding: 1px 6px;
  border-radius: 2px;
  color: var(--ink);
}
pre {
  background: var(--paper-off);
  border: 1px solid var(--rule);
  border-left: 3px solid var(--accent);
  padding: 18px 20px;
  overflow-x: auto;
  margin: 16px 0 20px;
  font-family: 'Source Code Pro', monospace;
  font-size: 0.82rem;
  line-height: 1.6;
  color: var(--ink);
}

/* ── ARCHITECTURE DIAGRAMS (SVG containers) ── */
.diagram-section {
  background: var(--paper-off);
  border: 1px solid var(--rule);
  padding: 28px 24px;
  margin: 20px 0 28px;
  overflow-x: auto;
}
.diagram-caption {
  font-family: 'Source Code Pro', monospace;
  font-size: 0.75rem;
  color: var(--ink-light);
  margin-top: 12px;
  text-align: center;
  letter-spacing: 0.05em;
}
.diagram-section svg { display: block; margin: 0 auto; }

/* ── COMPARISON TABLE ── */
.data-table {
  width: 100%;
  border-collapse: collapse;
  font-size: 0.85rem;
  margin: 16px 0 24px;
}
.data-table caption {
  font-family: 'Source Code Pro', monospace;
  font-size: 0.72rem;
  color: var(--ink-light);
  text-align: left;
  margin-bottom: 8px;
  letter-spacing: 0.05em;
  caption-side: bottom;
  padding-top: 8px;
}
.data-table th {
  background: var(--paper-off);
  border: 1px solid var(--rule);
  padding: 9px 14px;
  text-align: left;
  font-family: 'Source Code Pro', monospace;
  font-size: 0.72rem;
  text-transform: uppercase;
  letter-spacing: 0.1em;
  color: var(--ink-mid);
  font-weight: 600;
}
.data-table td {
  border: 1px solid var(--rule);
  padding: 9px 14px;
  vertical-align: top;
  line-height: 1.55;
  color: var(--ink-mid);
}
.data-table tr:nth-child(even) td { background: var(--paper-off); }
.data-table tr:hover td { background: var(--accent-lt); }
.data-table .feature { font-family: 'Source Code Pro', monospace; font-size: 0.8rem; color: var(--ink); }
.bad  { color: var(--red);    font-weight: 700; }
.warn { color: var(--amber);  font-weight: 700; }
.good { color: var(--green);  font-weight: 700; }
.best { color: var(--purple); font-weight: 700; }

/* ── SCORE BAR ── */
.scoreboard { margin: 20px 0 28px; }
.score-row { display: flex; align-items: center; gap: 14px; margin-bottom: 10px; font-size: 0.82rem; }
.score-label { width: 180px; flex-shrink: 0; font-family: 'Source Code Pro', monospace; font-size: 0.77rem; color: var(--ink-mid); }
.score-track { flex: 1; background: var(--paper-rule); height: 18px; border: 1px solid var(--rule); }
.score-fill { height: 100%; display: flex; align-items: center; padding-left: 8px; font-family: 'Source Code Pro', monospace; font-size: 0.68rem; font-weight: 600; white-space: nowrap; }
.score-val { width: 90px; flex-shrink: 0; font-family: 'Source Code Pro', monospace; font-size: 0.77rem; text-align: right; color: var(--ink-mid); }

/* ── PARAMETER DETAIL TABLE ── */
.param-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 16px 0 24px; }
.param-block { background: var(--paper-off); border: 1px solid var(--rule); padding: 18px 20px; }
.param-block h5 { font-family: 'Source Code Pro', monospace; font-size: 0.75rem; text-transform: uppercase; letter-spacing: 0.12em; color: var(--accent); margin-bottom: 12px; border-bottom: 1px solid var(--rule); padding-bottom: 6px; }
.param-row { display: flex; justify-content: space-between; font-size: 0.82rem; padding: 4px 0; border-bottom: 1px dotted var(--paper-rule); }
.param-row:last-child { border-bottom: none; }
.param-key { font-family: 'Source Code Pro', monospace; font-size: 0.78rem; color: var(--ink-mid); }
.param-val { font-family: 'Source Code Pro', monospace; font-size: 0.78rem; color: var(--ink); font-weight: 600; }

/* ── FOOTNOTES / REFERENCES ── */
.references ol { padding-left: 22px; }
.references li {
  font-size: 0.83rem;
  color: var(--ink-mid);
  line-height: 1.6;
  margin-bottom: 8px;
  font-family: 'Lato', sans-serif;
}
.references li span { font-family: 'Source Code Pro', monospace; font-size: 0.75rem; color: var(--ink-light); }

/* ── DIVIDER ── */
hr { border: none; border-top: 1px solid var(--rule); margin: 40px 0; }

/* ── PRINT ── */
@media print {
  .toc { float: none; width: 100%; margin: 0 0 32px 0; }
  .diagram-section svg { max-width: 100%; }
}

@media (max-width: 700px) {
  .toc { float: none; width: 100%; margin: 0 0 28px 0; }
  .param-grid { grid-template-columns: 1fr; }
}
</style>
</head>
<body>
<div class="page">

<!-- ══════════ HEADER ══════════ -->
<header class="doc-header">
  <div class="kicker">Technical Report &mdash; Deep Reinforcement Learning</div>
  <h1>Architecture Analysis and Upgrade Path for Humanoid-v4 Locomotion:<br>From Broken TD3 to State-of-the-Art CrossQ</h1>
  <p class="abstract">
    This document provides a detailed architectural analysis of three deep reinforcement learning
    configurations applied to the MuJoCo Humanoid-v4 continuous control benchmark: the original
    TD3/DDPG implementation (V3), a corrected SAC implementation (V4), and the state-of-the-art
    CrossQ architecture (Bhatt et al., ICLR 2024). We describe each component mathematically,
    enumerate failure modes, and provide a ranked upgrade roadmap.
  </p>
  <div class="doc-meta">
    Environment: <span>Humanoid-v4 (MuJoCo)</span> &nbsp;|&nbsp;
    Obs. Space: <span>376-dimensional continuous</span> &nbsp;|&nbsp;
    Action Space: <span>17-dimensional continuous</span> &nbsp;|&nbsp;
    Framework: <span>Stable-Baselines3</span>
  </div>
</header>

<!-- ══════════ TOC ══════════ -->
<div class="toc">
  <h4>Contents</h4>
  <ol>
    <li><a href="#s1">Problem Statement</a></li>
    <li><a href="#s2">V3 Architecture (TD3)</a></li>
    <li><a href="#s3">Failure Mode Analysis</a></li>
    <li><a href="#s4">V4 Architecture (SAC)</a></li>
    <li><a href="#s5">SOTA: CrossQ</a></li>
    <li><a href="#s6">Architecture Comparison</a></li>
    <li><a href="#s7">Upgrade Roadmap</a></li>
    <li><a href="#s8">References</a></li>
  </ol>
</div>

<!-- ══════════ S1: PROBLEM STATEMENT ══════════ -->
<section id="s1">
  <span class="section-num">Section 1</span>
  <h2>Problem Statement</h2>

  <p>
    The Humanoid-v4 task requires controlling a 17 degree-of-freedom bipedal robot to walk
    as fast as possible without falling. The observation vector is 376-dimensional and includes
    joint positions, velocities, actuator forces, and contact sensors. The reward function
    combines a forward velocity bonus, a survival bonus of +5 per step, and penalties for
    control effort and joint velocity. Episodes terminate either on falling (contact of the
    torso with the ground) or at 1000 steps via the <code>TimeLimit</code> wrapper.
  </p>

  <p>
    This task is among the hardest in the standard MuJoCo suite for model-free reinforcement
    learning. The combination of high-dimensional observation, many degrees of freedom, sparse
    locomotion signal in early training, and the requirement for coordinated multi-joint
    behavior means that algorithm choice and architectural stability are both critical.
    A naive or unstable algorithm will either collapse to a degenerate falling-forward policy
    or diverge numerically before meaningful locomotion is acquired.
  </p>

  <div class="callout note">
    <div class="callout-label">Benchmark Context</div>
    <p>Current state-of-the-art methods achieve approximately 6000&ndash;8000 reward at 1M
    environment steps on Humanoid-v4. A reward below 1000 corresponds to the agent falling
    immediately or moving minimally. A reward above 3000 indicates sustained bipedal locomotion.</p>
  </div>
</section>

<!-- ══════════ S2: V3 ARCHITECTURE ══════════ -->
<section id="s2">
  <span class="section-num">Section 2</span>
  <h2>V3 Architecture: Twin Delayed Deep Deterministic Policy Gradient (TD3)</h2>

  <p>
    The original implementation uses TD3 (Fujimoto et al., 2018), which itself is an improvement
    over DDPG (Lillicrap et al., 2016). TD3 introduces three modifications to DDPG: (1) twin
    critic networks with a minimum operator to reduce overestimation bias, (2) delayed actor
    updates relative to critic updates, and (3) target policy smoothing via noise injection.
    All three components are present in V3 but the surrounding infrastructure contains
    multiple critical defects.
  </p>

  <h3>2.1 Policy Network (Actor)</h3>
  <p>
    The actor is a deterministic function <em>mu</em>(s; theta) mapping observations to actions.
    It is parameterized as a multilayer perceptron:
  </p>
  <pre>Actor:
  Input:  s in R^376
  Layer1: Linear(376 -> 400), ReLU
  Layer2: Linear(400 -> 300), ReLU
  Output: Linear(300 -> 17),  tanh

  Action: a = tanh( W3 * ReLU( W2 * ReLU( W1 * s + b1 ) + b2 ) + b3 )</pre>

  <p>
    The <code>tanh</code> output squashes each action dimension to [-1, 1] to match the
    Humanoid's normalized action space. Exploration is achieved by injecting additive Gaussian
    noise at training time: <em>a_explore = clip( mu(s) + epsilon, -1, 1)</em> where
    <em>epsilon ~ N(0, sigma^2 * I)</em> with <em>sigma = 0.1</em>.
  </p>

  <div class="callout error">
    <div class="callout-label">Critical Defect: Deterministic Policy</div>
    <p>The deterministic actor provides no principled mechanism for exploration in high-dimensional
    continuous spaces. The noise injection is heuristic and does not adapt to the shape of the
    reward landscape. For Humanoid, this means the agent cannot efficiently explore the space of
    viable gait patterns and typically collapses to a locally optimal but degenerate strategy
    (falling in a controlled direction) within the first 50k steps.</p>
  </div>

  <h3>2.2 Value Networks (Critics)</h3>
  <p>
    TD3 maintains two independent critic networks Q1 and Q2, each mapping (state, action) pairs
    to scalar Q-value estimates. This twin structure addresses the overestimation bias of
    single-critic methods.
  </p>
  <pre>Critic (identical structure for Q1 and Q2):
  Input:  [s, a] in R^(376 + 17) = R^393
  Layer1: Linear(393 -> 400), ReLU
  Layer2: Linear(400 -> 300), ReLU
  Output: Linear(300 -> 1)   [scalar Q-value]

  Q_target = r + gamma * min(Q1_target(s', a'_noisy), Q2_target(s', a'_noisy))
  where a'_noisy = clip( mu_target(s') + clip(N(0,0.2^2), -0.5, 0.5), -1, 1 )</pre>

  <p>
    The target policy smoothing (epsilon clipped to [-0.5, 0.5] with standard deviation 0.2)
    regularizes the critic update and prevents it from exploiting sharp peaks in the Q-function
    estimate. The actor is updated every two critic steps (<code>policy_delay=2</code>),
    which allows the critic to stabilize before the actor gradient is computed.
  </p>

  <h3>2.3 Replay Buffer</h3>
  <p>
    Experience is stored in a first-in-first-out ring buffer of capacity 300,000 transitions.
    Each transition stores <em>(s, a, r, s', done)</em>. With 32-bit floats:
  </p>
  <pre>Memory per transition = (376 + 17 + 1 + 376 + 1) floats * 4 bytes = 3,084 bytes

At 300k transitions:  300,000 * 3,084 = ~925 MB
At 1M  transitions:   if buffer = total_steps -> ~3.1 GB (before Python overhead)
Actual observed peak: ~5-7 GB (Python object overhead + gradient computation)</pre>

  <p>
    The original code sets <code>buffer_size = total_timesteps = 300_000</code>, which couples
    buffer memory to training duration. Increasing <code>total_timesteps</code> without adjusting
    <code>buffer_size</code> causes an out-of-memory crash on systems with under 8 GB available RAM.
  </p>

  <h3>2.4 Target Networks and Soft Update</h3>
  <p>
    Four target networks are maintained: a target actor <em>mu'</em> and two target critics
    <em>Q1', Q2'</em>. These are updated via Polyak averaging after every critic gradient step:
  </p>
  <pre>theta_target &lt;- tau * theta + (1 - tau) * theta_target
where tau = 0.005</pre>

  <p>
    Target networks prevent the moving-target problem where the TD backup target and the
    value estimate being updated share parameters. The lag introduced by <em>tau = 0.005</em>
    means the target changes by at most 0.5% per step, providing a stable regression target.
  </p>

  <!-- V3 ARCHITECTURE DIAGRAM -->
  <div class="diagram-section">
    <svg viewBox="0 0 980 380" width="940" xmlns="http://www.w3.org/2000/svg" style="font-family:'Source Code Pro',monospace;">
      <defs>
        <marker id="a1" markerWidth="8" markerHeight="6" refX="7" refY="3" orient="auto"><path d="M0,0 L8,3 L0,6 Z" fill="#888"/></marker>
        <marker id="a-red" markerWidth="8" markerHeight="6" refX="7" refY="3" orient="auto"><path d="M0,0 L8,3 L0,6 Z" fill="#8a2020"/></marker>
      </defs>

      <!-- OBSERVATION -->
      <rect x="10" y="155" width="115" height="70" fill="#f2f1ec" stroke="#c8c4b4" stroke-width="1.2"/>
      <text x="67" y="182" text-anchor="middle" font-size="10" font-weight="600" fill="#1a1a1a">Observation</text>
      <text x="67" y="198" text-anchor="middle" font-size="9" fill="#777">s in R^376</text>
      <text x="67" y="213" text-anchor="middle" font-size="9" fill="#aaa">VecNormalize</text>

      <!-- ACTOR BOX -->
      <rect x="165" y="30" width="210" height="155" fill="#fdf5f5" stroke="#c8a0a0" stroke-width="1.2"/>
      <text x="270" y="55" text-anchor="middle" font-size="11" font-weight="600" fill="#5a1010">Actor (Deterministic)</text>
      <line x1="175" y1="62" x2="365" y2="62" stroke="#e8d0d0" stroke-width="0.8"/>
      <text x="270" y="80" text-anchor="middle" font-size="9" fill="#7a3030">Linear(376, 400)</text>
      <text x="270" y="96" text-anchor="middle" font-size="9" fill="#aaa">ReLU activation</text>
      <text x="270" y="112" text-anchor="middle" font-size="9" fill="#7a3030">Linear(400, 300)</text>
      <text x="270" y="128" text-anchor="middle" font-size="9" fill="#aaa">ReLU activation</text>
      <text x="270" y="144" text-anchor="middle" font-size="9" fill="#7a3030">Linear(300, 17)</text>
      <text x="270" y="160" text-anchor="middle" font-size="9" fill="#aaa">tanh squashing</text>
      <rect x="175" y="167" width="180" height="12" fill="#f5e8e8" stroke="#c8a0a0" stroke-width="0.8"/>
      <text x="270" y="177" text-anchor="middle" font-size="7.5" fill="#8a2020">No entropy regularization -- exploration by noise only</text>

      <!-- ACTION OUT -->
      <rect x="418" y="70" width="100" height="50" fill="#f2f1ec" stroke="#c8c4b4" stroke-width="1.2"/>
      <text x="468" y="91" text-anchor="middle" font-size="9.5" font-weight="600" fill="#1a1a1a">Action</text>
      <text x="468" y="107" text-anchor="middle" font-size="9" fill="#777">a in R^17</text>

      <!-- CRITIC 1 -->
      <rect x="165" y="210" width="210" height="80" fill="#f5faf5" stroke="#a0c0a0" stroke-width="1.2"/>
      <text x="270" y="232" text-anchor="middle" font-size="11" font-weight="600" fill="#1a4a1a">Critic Q1</text>
      <text x="270" y="250" text-anchor="middle" font-size="9" fill="#3a6a3a">Linear(393, 400) -- ReLU</text>
      <text x="270" y="266" text-anchor="middle" font-size="9" fill="#3a6a3a">Linear(400, 300) -- ReLU</text>
      <text x="270" y="282" text-anchor="middle" font-size="9" fill="#3a6a3a">Linear(300, 1) -&gt; Q1(s,a)</text>

      <!-- CRITIC 2 -->
      <rect x="165" y="305" width="210" height="65" fill="#f5faf5" stroke="#a0c0a0" stroke-width="1.2"/>
      <text x="270" y="327" text-anchor="middle" font-size="11" font-weight="600" fill="#1a4a1a">Critic Q2 (identical)</text>
      <text x="270" y="345" text-anchor="middle" font-size="9" fill="#3a6a3a">Linear(393, 400) -- ReLU</text>
      <text x="270" y="361" text-anchor="middle" font-size="9" fill="#3a6a3a">Linear(400, 300) -- ReLU -- Linear(300,1)</text>

      <!-- MIN Q -->
      <rect x="418" y="235" width="100" height="50" fill="#f5faf5" stroke="#a0c0a0" stroke-width="1.2"/>
      <text x="468" y="257" text-anchor="middle" font-size="9.5" font-weight="600" fill="#1a4a1a">min(Q1, Q2)</text>
      <text x="468" y="273" text-anchor="middle" font-size="8.5" fill="#777">TD target</text>

      <!-- TARGET NETWORKS -->
      <rect x="560" y="120" width="170" height="120" fill="#f8f8fa" stroke="#c0c0d8" stroke-width="1" stroke-dasharray="5,3"/>
      <text x="645" y="143" text-anchor="middle" font-size="10" fill="#4a4a7a">Target Networks</text>
      <text x="645" y="160" text-anchor="middle" font-size="8.5" fill="#888">mu_target  (copy of actor)</text>
      <text x="645" y="176" text-anchor="middle" font-size="8.5" fill="#888">Q1_target  (copy of Q1)</text>
      <text x="645" y="192" text-anchor="middle" font-size="8.5" fill="#888">Q2_target  (copy of Q2)</text>
      <text x="645" y="210" text-anchor="middle" font-size="8.5" fill="#aaa">tau = 0.005 soft update</text>
      <text x="645" y="228" text-anchor="middle" font-size="8.5" fill="#aaa">policy_delay = 2</text>

      <!-- REPLAY BUFFER -->
      <rect x="560" y="265" width="170" height="90" fill="#fdf5f5" stroke="#c8a0a0" stroke-width="1.2"/>
      <text x="645" y="287" text-anchor="middle" font-size="10" fill="#5a1010">Replay Buffer</text>
      <text x="645" y="305" text-anchor="middle" font-size="8.5" fill="#7a3030">capacity: 300,000</text>
      <text x="645" y="321" text-anchor="middle" font-size="8" fill="#8a2020">~925 MB (underestimate)</text>
      <text x="645" y="337" text-anchor="middle" font-size="8" fill="#8a2020">OOM if steps > 300k</text>
      <text x="645" y="351" text-anchor="middle" font-size="8" fill="#8a2020">no optimize_memory_usage</text>

      <!-- FAILURE LABELS -->
      <rect x="760" y="30" width="205" height="330" fill="#fdf5f5" stroke="#c8a0a0" stroke-width="1"/>
      <text x="862" y="52" text-anchor="middle" font-size="10" font-weight="600" fill="#5a1010">Identified Failure Modes</text>
      <line x1="770" y1="58" x2="958" y2="58" stroke="#e0c0c0" stroke-width="0.8"/>
      <text x="772" y="76" font-size="8.5" fill="#7a3030">[F1] No gradient norm clipping</text>
      <text x="772" y="92" font-size="8" fill="#aaa">     critic loss diverges to inf</text>
      <text x="772" y="110" font-size="8.5" fill="#7a3030">[F2] Buffer OOM at 1M steps</text>
      <text x="772" y="126" font-size="8" fill="#aaa">     ~5-7 GB actual usage</text>
      <text x="772" y="144" font-size="8.5" fill="#7a3030">[F3] clip_obs=10, too permissive</text>
      <text x="772" y="160" font-size="8" fill="#aaa">     NaN obs after 500k steps</text>
      <text x="772" y="178" font-size="8.5" fill="#7a3030">[F4] Adam eps=1e-8 (default)</text>
      <text x="772" y="194" font-size="8" fill="#aaa">     divide by ~0 in late train</text>
      <text x="772" y="212" font-size="8.5" fill="#7a3030">[F5] Deterministic policy</text>
      <text x="772" y="228" font-size="8" fill="#aaa">     collapses to local optima</text>
      <text x="772" y="246" font-size="8.5" fill="#7a3030">[F6] Net too narrow [400,300]</text>
      <text x="772" y="262" font-size="8" fill="#aaa">     insufficient Q-fn capacity</text>
      <text x="772" y="280" font-size="8.5" fill="#7a3030">[F7] gradient_steps=1 (default)</text>
      <text x="772" y="296" font-size="8" fill="#aaa">     low sample efficiency</text>
      <line x1="770" y1="310" x2="958" y2="310" stroke="#e0c0c0" stroke-width="0.8"/>
      <text x="862" y="327" text-anchor="middle" font-size="8.5" fill="#7a3030">Observed reward: 300-800</text>
      <text x="862" y="345" text-anchor="middle" font-size="8" fill="#aaa">Crashes before 500k steps</text>

      <!-- ARROWS -->
      <line x1="125" y1="182" x2="163" y2="120" stroke="#888" stroke-width="1.2" marker-end="url(#a1)"/>
      <line x1="125" y1="195" x2="163" y2="248" stroke="#888" stroke-width="1.2" marker-end="url(#a1)"/>
      <line x1="125" y1="200" x2="163" y2="335" stroke="#888" stroke-width="1.2" marker-end="url(#a1)"/>
      <line x1="375" y1="100" x2="416" y2="95" stroke="#888" stroke-width="1.2" marker-end="url(#a1)"/>
      <line x1="375" y1="257" x2="416" y2="260" stroke="#888" stroke-width="1.2" marker-end="url(#a1)"/>
      <line x1="518" y1="260" x2="558" y2="195" stroke="#888" stroke-width="1.2" marker-end="url(#a1)"/>
      <line x1="518" y1="280" x2="558" y2="300" stroke="#888" stroke-width="1.2" marker-end="url(#a1)"/>
    </svg>
    <div class="diagram-caption">Figure 1. V3 Architecture (TD3/DDPG). Red annotations indicate failure modes. Dashed box indicates target networks. Input to critics concatenates [s, a].</div>
  </div>
</section>

<!-- ══════════ S3: FAILURE MODES ══════════ -->
<section id="s3">
  <span class="section-num">Section 3</span>
  <h2>Failure Mode Analysis</h2>

  <h3>3.1 [F1] Replay Buffer Out-of-Memory</h3>
  <p>
    The most immediate crash cause at large iteration counts. The original code sets
    <code>buffer_size = total_timesteps = 300_000</code>, directly coupling memory consumption to
    training duration. Each stored transition holds both <em>s</em> and <em>s'</em> as full
    copies of the 376-dimensional observation. Without <code>optimize_memory_usage=True</code>,
    SB3's <code>ReplayBuffer</code> pre-allocates two separate observation arrays.
  </p>
  <pre>Storage without optimization:
  obs      array: (buffer_size, 376) float32  = 300k * 376 * 4 = 451 MB
  next_obs array: (buffer_size, 376) float32  = 300k * 376 * 4 = 451 MB
  actions  array: (buffer_size, 17)  float32  = 300k * 17  * 4 = 20 MB
  rewards  array: (buffer_size, 1)   float32  = 300k * 1   * 4 = 1.2 MB
  dones    array: (buffer_size, 1)   float32  = 300k * 1   * 4 = 1.2 MB
  Total pre-allocation (estimated): ~925 MB

  With optimize_memory_usage=True:
  next_obs stored as shifted index into obs array (no second copy)
  Total: ~475 MB -- approximately half</pre>

  <div class="callout error">
    <div class="callout-label">Root Cause</div>
    <p>At 1M total timesteps with <code>buffer_size = 1_000_000</code> (implied by the original
    coupling), pre-allocation reaches ~3.1 GB before any model parameters or gradient buffers
    are counted. Combined with PyTorch model weights, gradients, and Python overhead, this
    exceeds Kaggle's 16 GB RAM limit reliably.</p>
  </div>

  <h3>3.2 [F2] Critic Gradient Explosion</h3>
  <p>
    Without gradient norm clipping, TD3's critic loss can grow unboundedly over long training
    runs. The critic is updated via mean-squared Bellman error:
  </p>
  <pre>L_critic = E[(Q(s,a) - y)^2]  where y = r + gamma * min(Q1'(s',a'_noisy), Q2'(s',a'_noisy))

dL/dW_i can grow as: || dL/dW_i || proportional to || Q(s,a) - y || * || d_Q/d_W_i ||

If Q-values drift (common in Humanoid due to large reward variance), the Bellman error
magnifies multiplicatively through backpropagation layers. Without clipping:
  - Gradient norm grows exponentially over training steps
  - Weight updates destabilize the network
  - Q-values become NaN within a few thousand steps of divergence</pre>

  <h3>3.3 [F3] VecNormalize Numerical Drift</h3>
  <p>
    VecNormalize maintains online estimates of observation mean and variance using Welford's
    algorithm. These estimates are theoretically stable but can exhibit numerical drift over
    very long training runs, particularly when the environment reward distribution shifts as
    the policy improves. With <code>clip_obs=10.0</code>, observations that are 10 standard
    deviations from the running mean pass through unmodified, which can produce extreme
    values in the normalized observation fed to the network. The fix is to tighten this to
    <code>clip_obs=5.0</code> and also apply <code>clip_reward=10.0</code>.
  </p>

  <h3>3.4 [F4] Adam Optimizer Numerical Instability</h3>
  <p>
    The Adam optimizer maintains per-parameter exponential moving averages of both the
    gradient (m) and the squared gradient (v). The parameter update is:
  </p>
  <pre>theta &lt;- theta - lr * m_hat / (sqrt(v_hat) + eps)

Default eps = 1e-8. In late-stage training, for parameters with consistently small
gradients, v_hat approaches zero. The denominator (sqrt(v_hat) + eps) can then be
dominated by floating-point noise, producing large or NaN updates.

Fix: eps = 1e-5 provides a more conservative floor, preventing divide-by-near-zero
without meaningfully affecting the update dynamics in normal operating conditions.</pre>

  <h3>3.5 [F5] handle_timeout_termination Conflict</h3>
  <p>
    This is the crash the user encountered directly. SAC in SB3 enables
    <code>handle_timeout_termination=True</code> by default in its replay buffer.
    This flag causes the buffer to track whether episodes ended due to a true terminal state
    or a <code>TimeLimit</code> truncation, and corrects the TD backup accordingly.
    However, SB3's implementation of <code>optimize_memory_usage=True</code> uses a circular
    buffer indexing trick that stores <em>next_obs</em> as <em>obs[t+1]</em> rather than as a
    separate copy, which is incompatible with the timeout tracking implementation.
  </p>
  <div class="callout good">
    <div class="callout-label">Fix Applied in V4</div>
    <p>Set <code>replay_buffer_kwargs=dict(handle_timeout_termination=False)</code>. This is
    safe for MuJoCo environments because Humanoid-v4 episodes end exclusively via the
    <code>TimeLimit</code> wrapper (truncation), never via true terminal states within the
    episode horizon. The TD backup target does not need correction for truncation when
    the critic's target includes the full discount chain.</p>
  </div>
</section>

<!-- ══════════ S4: V4 ARCHITECTURE ══════════ -->
<section id="s4">
  <span class="section-num">Section 4</span>
  <h2>V4 Architecture: Soft Actor-Critic (SAC)</h2>

  <p>
    SAC (Haarnoja et al., 2018) replaces the deterministic TD3 objective with a maximum
    entropy framework. The agent maximizes expected return augmented by the entropy of the
    policy at each visited state:
  </p>
  <pre>J(pi) = sum_t E[(r(s_t, a_t) + alpha * H(pi(. | s_t)))]

where H(pi(.|s)) = -E_a[log pi(a|s)] is the entropy of the policy,
and alpha > 0 is the temperature parameter trading off reward vs. exploration.</pre>

  <p>
    This entropy bonus has a critical practical effect for Humanoid: it prevents the policy
    from collapsing to deterministic degenerate strategies (e.g., controlled falling) by
    maintaining pressure toward diverse action distributions throughout training.
    Haarnoja et al. (2018) show that entropy maximization leads to naturally multi-modal
    policies that can recover from perturbations and generalize better.
  </p>

  <h3>4.1 Stochastic Actor</h3>
  <pre>Actor network f(s; phi):
  Input:  s in R^376
  Layer1: Linear(376, 512), ReLU
  Layer2: Linear(512, 512), ReLU
  Output: two heads in parallel:
          mu_head:    Linear(512, 17)    [mean of Gaussian]
          logstd_head: Linear(512, 17)  [log standard deviation, clamped to [-20, 2]]

  Action sampling (reparameterization trick):
          z ~ N(mu(s), exp(logstd(s))^2)
          a = tanh(z)

  Log probability (used for entropy computation):
          log pi(a|s) = log N(z | mu, std) - sum(log(1 - tanh(z)^2))</pre>

  <p>
    The reparameterization trick allows gradients to flow through the sampled action back
    to the actor parameters, making the entropy-augmented objective differentiable.
    The Jacobian correction term <code>log(1 - tanh(z)^2)</code> accounts for the change
    of variables from Gaussian samples to bounded actions.
  </p>

  <h3>4.2 Automatic Entropy Tuning</h3>
  <p>
    Rather than treating alpha as a fixed hyperparameter, SAC V4 automatically adjusts
    it via a dual optimization objective. A target entropy H_target is set to the negative
    of the action dimension (a heuristic from Haarnoja et al. 2018):
  </p>
  <pre>H_target = -dim(A) = -17

Dual objective for alpha:
  L(alpha) = E_a[-alpha * (log pi(a|s) + H_target)]

Alpha is updated via gradient descent on L(alpha) after each critic update.
If current entropy > H_target: alpha decreases (less exploration pressure)
If current entropy < H_target: alpha increases (more exploration pressure)</pre>

  <h3>4.3 Twin Soft Critics</h3>
  <pre>SAC Critic (two identical networks):
  Input:  [s, a] in R^393
  Layer1: Linear(393, 512), ReLU
  Layer2: Linear(512, 512), ReLU
  Output: Linear(512, 1) -> Q(s,a)

  Soft Bellman target:
  y = r + gamma * (min(Q1'(s',a'), Q2'(s',a')) - alpha * log pi(a'|s'))
  where a' ~ pi(.|s')  [sampled from current policy, not target policy]

  Critic loss (per network):
  L_Q = E[(Q(s,a) - y)^2]</pre>

  <p>
    The key difference from TD3 is the entropy correction in the Bellman target:
    <code>- alpha * log pi(a'|s')</code>. This subtracts the log probability of the next
    action from the backup target, effectively penalizing overconfident (low-entropy)
    next-state policies and encouraging the critic to assign higher value to state-action
    pairs reachable via diverse policies.
  </p>

  <!-- V4 DIAGRAM -->
  <div class="diagram-section">
    <svg viewBox="0 0 980 420" width="940" xmlns="http://www.w3.org/2000/svg" style="font-family:'Source Code Pro',monospace;">
      <defs>
        <marker id="a2" markerWidth="8" markerHeight="6" refX="7" refY="3" orient="auto"><path d="M0,0 L8,3 L0,6 Z" fill="#2a4a8a"/></marker>
      </defs>

      <!-- OBS -->
      <rect x="10" y="185" width="120" height="70" fill="#f2f1ec" stroke="#c8c4b4" stroke-width="1.2"/>
      <text x="70" y="212" text-anchor="middle" font-size="10" font-weight="600" fill="#1a1a1a">Observation</text>
      <text x="70" y="228" text-anchor="middle" font-size="9" fill="#777">s in R^376</text>
      <text x="70" y="244" text-anchor="middle" font-size="8.5" fill="#2a4a8a">clip_obs=5 [FIXED]</text>

      <!-- STOCHASTIC ACTOR -->
      <rect x="168" y="30" width="220" height="195" fill="#f5faf2" stroke="#90b880" stroke-width="1.5"/>
      <text x="278" y="55" text-anchor="middle" font-size="11" font-weight="600" fill="#1a4a0a">Stochastic Actor</text>
      <line x1="178" y1="62" x2="378" y2="62" stroke="#c0d8a0" stroke-width="0.8"/>
      <text x="278" y="80" text-anchor="middle" font-size="9" fill="#3a6a1a">Linear(376, 512)</text>
      <text x="278" y="96" text-anchor="middle" font-size="8.5" fill="#aaa">ReLU  [wider: +28% vs V3]</text>
      <text x="278" y="113" text-anchor="middle" font-size="9" fill="#3a6a1a">Linear(512, 512)</text>
      <text x="278" y="129" text-anchor="middle" font-size="8.5" fill="#aaa">ReLU</text>
      <line x1="178" y1="138" x2="378" y2="138" stroke="#c0d8a0" stroke-width="0.8" stroke-dasharray="4,2"/>
      <text x="230" y="156" text-anchor="middle" font-size="9" fill="#3a6a1a">mu head (17)</text>
      <text x="330" y="156" text-anchor="middle" font-size="9" fill="#3a6a1a">logstd head (17)</text>
      <line x1="178" y1="163" x2="378" y2="163" stroke="#c0d8a0" stroke-width="0.8"/>
      <text x="278" y="180" text-anchor="middle" font-size="9" fill="#3a6a1a">z ~ N(mu, exp(logstd)^2)</text>
      <text x="278" y="196" text-anchor="middle" font-size="9" fill="#3a6a1a">a = tanh(z)  [reparameterized]</text>
      <rect x="178" y="206" width="200" height="12" fill="#e4f0d8" stroke="#90b880"/>
      <text x="278" y="215" text-anchor="middle" font-size="7.5" fill="#1a4a0a">Entropy: H = -E[log pi(a|s)]  auto-tuned alpha</text>

      <!-- AUTO ALPHA -->
      <rect x="428" y="30" width="150" height="80" fill="#eef0f8" stroke="#a0a8d0" stroke-width="1.2"/>
      <text x="503" y="53" text-anchor="middle" font-size="10" font-weight="600" fill="#2a3a7a">Auto-alpha (alpha)</text>
      <text x="503" y="72" text-anchor="middle" font-size="8.5" fill="#4a5a9a">H_target = -dim(A) = -17</text>
      <text x="503" y="88" text-anchor="middle" font-size="8.5" fill="#4a5a9a">L(alpha) = alpha*(H - H_target)</text>
      <text x="503" y="104" text-anchor="middle" font-size="8" fill="#2a4a8a">gradient descent on alpha</text>

      <!-- ACTION -->
      <rect x="428" y="140" width="150" height="55" fill="#f2f1ec" stroke="#c8c4b4" stroke-width="1.2"/>
      <text x="503" y="162" text-anchor="middle" font-size="10" font-weight="600" fill="#1a1a1a">Sampled Action</text>
      <text x="503" y="178" text-anchor="middle" font-size="9" fill="#777">a in R^17</text>
      <text x="503" y="193" text-anchor="middle" font-size="8" fill="#aaa">+ log pi(a|s) for entropy</text>

      <!-- SAC CRITICS -->
      <rect x="168" y="250" width="220" height="110" fill="#f2f8fa" stroke="#90b8c8" stroke-width="1.5"/>
      <text x="278" y="273" text-anchor="middle" font-size="11" font-weight="600" fill="#0a3a4a">SAC Critic Q1, Q2</text>
      <line x1="178" y1="280" x2="378" y2="280" stroke="#b0d0dc" stroke-width="0.8"/>
      <text x="278" y="297" text-anchor="middle" font-size="9" fill="#1a5a6a">Linear(393, 512), ReLU</text>
      <text x="278" y="313" text-anchor="middle" font-size="9" fill="#1a5a6a">Linear(512, 512), ReLU</text>
      <text x="278" y="329" text-anchor="middle" font-size="9" fill="#1a5a6a">Linear(512, 1) -> Q(s,a)</text>
      <rect x="178" y="338" width="200" height="14" fill="#e0f0f4" stroke="#90b8c8"/>
      <text x="278" y="349" text-anchor="middle" font-size="7.5" fill="#0a3a4a">max_grad_norm=10, Adam eps=1e-5 [FIXED]</text>
      <text x="278" y="362" text-anchor="middle" font-size="7.5" fill="#0a3a4a">Soft target: Q1', Q2' tau=0.005</text>

      <!-- SOFT BELLMAN -->
      <rect x="428" y="250" width="150" height="80" fill="#f2f8fa" stroke="#90b8c8" stroke-width="1.2"/>
      <text x="503" y="271" text-anchor="middle" font-size="9.5" font-weight="600" fill="#0a3a4a">Soft Bellman Target</text>
      <text x="503" y="289" text-anchor="middle" font-size="8" fill="#1a5a6a">y = r + gamma *</text>
      <text x="503" y="305" text-anchor="middle" font-size="8" fill="#1a5a6a">(min(Q1',Q2')(s',a')</text>
      <text x="503" y="321" text-anchor="middle" font-size="8" fill="#1a5a6a"> - alpha * log pi(a'|s'))</text>

      <!-- BUFFER FIXED -->
      <rect x="428" y="348" width="150" height="60" fill="#f5faf2" stroke="#90b880" stroke-width="1.2"/>
      <text x="503" y="369" text-anchor="middle" font-size="9.5" font-weight="600" fill="#1a4a0a">Buffer [FIXED]</text>
      <text x="503" y="386" text-anchor="middle" font-size="8" fill="#3a6a1a">cap 500k, optimize_memory=T</text>
      <text x="503" y="402" text-anchor="middle" font-size="8" fill="#3a6a1a">~1.7 GB (was ~5-7 GB)</text>

      <!-- FIXES SUMMARY -->
      <rect x="615" y="30" width="350" height="380" fill="#f5faf2" stroke="#90b880" stroke-width="1"/>
      <text x="790" y="55" text-anchor="middle" font-size="10" font-weight="600" fill="#1a4a0a">Changes from V3 to V4</text>
      <line x1="625" y1="62" x2="958" y2="62" stroke="#c0d8a0" stroke-width="0.8"/>
      <text x="627" y="80" font-size="8.5" fill="#2a6a1a">[C1] TD3 -> SAC</text>
      <text x="627" y="96" font-size="8" fill="#777">     deterministic -> stochastic policy</text>
      <text x="627" y="112" font-size="8" fill="#777">     entropy term in Bellman target</text>
      <text x="627" y="130" font-size="8.5" fill="#2a6a1a">[C2] Network width [400,300] -> [512,512]</text>
      <text x="627" y="146" font-size="8" fill="#777">     +60% parameters in critic</text>
      <text x="627" y="164" font-size="8.5" fill="#2a6a1a">[C3] Gradient norm clipping: None -> 10.0</text>
      <text x="627" y="180" font-size="8" fill="#777">     prevents critic divergence</text>
      <text x="627" y="198" font-size="8.5" fill="#2a6a1a">[C4] Adam eps: 1e-8 -> 1e-5</text>
      <text x="627" y="214" font-size="8" fill="#777">     prevents divide-by-near-zero</text>
      <text x="627" y="232" font-size="8.5" fill="#2a6a1a">[C5] Buffer: 300k -> 500k cap</text>
      <text x="627" y="248" font-size="8" fill="#777">     + optimize_memory_usage=True</text>
      <text x="627" y="264" font-size="8" fill="#777">     + handle_timeout=False</text>
      <text x="627" y="280" font-size="8" fill="#777">     RAM: 5-7 GB -> ~1.7 GB</text>
      <text x="627" y="298" font-size="8.5" fill="#2a6a1a">[C6] clip_obs: 10 -> 5</text>
      <text x="627" y="314" font-size="8" fill="#777">     + clip_reward=10 added</text>
      <text x="627" y="332" font-size="8.5" fill="#2a6a1a">[C7] OOM guard (psutil check)</text>
      <text x="627" y="348" font-size="8" fill="#777">     warns if RAM < 4 GB before start</text>
      <line x1="625" y1="360" x2="958" y2="360" stroke="#c0d8a0" stroke-width="0.8"/>
      <text x="790" y="377" text-anchor="middle" font-size="8.5" fill="#1a4a0a">Expected: 2,000-5,000 reward @ 1M steps</text>
      <text x="790" y="393" text-anchor="middle" font-size="8" fill="#aaa">Remaining gap: no BatchNorm, target nets present</text>

      <!-- ARROWS -->
      <line x1="130" y1="210" x2="166" y2="140" stroke="#2a4a8a" stroke-width="1.2" marker-end="url(#a2)"/>
      <line x1="130" y1="220" x2="166" y2="300" stroke="#2a4a8a" stroke-width="1.2" marker-end="url(#a2)"/>
      <line x1="388" y1="90" x2="426" y2="75" stroke="#2a4a8a" stroke-width="1.2" marker-end="url(#a2)"/>
      <line x1="388" y1="175" x2="426" y2="168" stroke="#2a4a8a" stroke-width="1.2" marker-end="url(#a2)"/>
      <line x1="388" y1="310" x2="426" y2="295" stroke="#2a4a8a" stroke-width="1.2" marker-end="url(#a2)"/>
      <line x1="388" y1="355" x2="426" y2="375" stroke="#2a4a8a" stroke-width="1.2" marker-end="url(#a2)"/>
    </svg>
    <div class="diagram-caption">Figure 2. V4 Architecture (SAC, fixed). Blue annotations indicate applied fixes. The stochastic actor now has two output heads for mean and log-standard-deviation. Auto-alpha tunes entropy coefficient during training.</div>
  </div>

  <div class="callout warn">
    <div class="callout-label">Remaining Limitations in V4</div>
    <p>V4 still uses vanilla MLP critics without normalization, and maintains target networks
    via soft updates. This prevents safe use of high update-to-data (UTD) ratios above 1.
    Setting <code>gradient_steps > 1</code> without architectural stabilization causes
    Q-value divergence, limiting sample efficiency.</p>
  </div>
</section>

<!-- ══════════ S5: CROSSQ ══════════ -->
<section id="s5">
  <span class="section-num">Section 5</span>
  <h2>State-of-the-Art: CrossQ (Bhatt et al., ICLR 2024)</h2>

  <p>
    CrossQ (Bhatt et al., 2024, arXiv:2301.02328) makes two architectural changes to SAC that
    together achieve 5,500&ndash;6,500 reward on Humanoid-v4 at 1M steps: (1) Batch Normalization
    is inserted between each linear layer of the critic networks, and (2) target networks are
    removed entirely. These two changes are causally related: BatchNorm stabilizes the
    Q-function scale sufficiently that the conservative lag provided by target networks
    is no longer necessary for training stability.
  </p>

  <h3>5.1 Batch Normalization in the Critic</h3>
  <p>
    BatchNorm (Ioffe & Szegedy, 2015) normalizes each mini-batch of activations to zero mean
    and unit variance, then applies learned affine parameters (gamma, beta):
  </p>
  <pre>For a mini-batch B = {x_1, ..., x_m} of activations at layer l:
  mu_B    = (1/m) * sum(x_i)
  var_B   = (1/m) * sum((x_i - mu_B)^2)
  x_hat_i = (x_i - mu_B) / sqrt(var_B + epsilon)
  y_i     = gamma * x_hat_i + beta          [gamma, beta learnable]

CrossQ Critic structure:
  Input:  [s, a] in R^393
  Linear(393, 1024)
  BatchNorm1d(1024)   &lt;-- KEY ADDITION
  ReLU
  Linear(1024, 1024)
  BatchNorm1d(1024)   &lt;-- KEY ADDITION
  ReLU
  Linear(1024, 1)     -&gt; Q(s,a)</pre>

  <p>
    The effect of BatchNorm on Q-function training stability is significant. In vanilla
    deep RL critics, Q-value scale can drift as the policy improves: early in training
    expected returns are low, but as locomotion is acquired they increase by orders of magnitude.
    Without normalization, the output layer weights must adapt to this changing scale, which
    causes the gradient magnitudes in earlier layers to oscillate. BatchNorm pins the
    intermediate activation distributions, decoupling the scale of the Q-function output
    from the intermediate representation quality.
  </p>

  <div class="callout paper">
    <div class="callout-label">Theoretical Justification (Bhatt et al., 2024)</div>
    <p>The paper proves that BatchNorm in the critic implicitly implements a form of
    Lipschitz regularization on the Q-function, bounding the rate at which Q-values can
    change between consecutive training steps. This bounded rate of change is what makes
    target networks redundant: the Q-function itself behaves like a slowly-moving target.</p>
  </div>

  <h3>5.2 Removal of Target Networks</h3>
  <p>
    Standard SAC and TD3 require target networks to prevent the "moving target" problem in
    TD learning. When both the current Q-function and the TD target are updated simultaneously,
    the optimization can become unstable (similar to chasing a moving reference). CrossQ
    shows that BatchNorm alone provides sufficient stability to eliminate target networks
    without introducing divergence.
  </p>
  <pre>Standard SAC Bellman target (uses target networks Q1', Q2'):
  y = r + gamma * (min(Q1'(s',a'), Q2'(s',a')) - alpha * log pi(a'|s'))

CrossQ Bellman target (uses current networks directly):
  y = r + gamma * (min(Q1(s',a'), Q2(s',a')) - alpha * log pi(a'|s'))
                       ^^^^^^^^^^^^^^^^^
                       No prime -- same network being trained</pre>

  <p>
    Removing target networks has two practical benefits: (a) the network sees the most
    current Q-estimates in the backup, accelerating convergence, and (b) memory is reduced
    by eliminating the four copies of network weights maintained for targets.
  </p>

  <h3>5.3 High Update-to-Data (UTD) Ratio</h3>
  <p>
    Because BatchNorm stabilizes the critic, CrossQ can safely use
    <code>gradient_steps = 4</code> with <code>train_freq = 1</code>, meaning four gradient
    updates are performed per environment step. This 4x UTD ratio effectively multiplies
    sample efficiency without additional environment interactions. In V3 and vanilla V4,
    UTD > 1 causes rapid Q-value divergence due to the unregularized critic.
  </p>

  <!-- CROSSQ DIAGRAM -->
  <div class="diagram-section">
    <svg viewBox="0 0 980 460" width="940" xmlns="http://www.w3.org/2000/svg" style="font-family:'Source Code Pro',monospace;">
      <defs>
        <marker id="a3" markerWidth="8" markerHeight="6" refX="7" refY="3" orient="auto"><path d="M0,0 L8,3 L0,6 Z" fill="#1a5030"/></marker>
      </defs>

      <!-- OBS -->
      <rect x="10" y="195" width="120" height="70" fill="#f2f1ec" stroke="#c8c4b4" stroke-width="1.2"/>
      <text x="70" y="222" text-anchor="middle" font-size="10" font-weight="600" fill="#1a1a1a">Observation</text>
      <text x="70" y="238" text-anchor="middle" font-size="9" fill="#777">s in R^376</text>
      <text x="70" y="254" text-anchor="middle" font-size="8.5" fill="#1a5030">clip_obs=5</text>

      <!-- SAC ACTOR (same as V4) -->
      <rect x="168" y="30" width="210" height="155" fill="#f5faf2" stroke="#90b880" stroke-width="1.2"/>
      <text x="273" y="53" text-anchor="middle" font-size="10.5" font-weight="600" fill="#1a4a0a">Stochastic Actor</text>
      <text x="273" y="69" text-anchor="middle" font-size="8.5" fill="#aaa">(same as V4 SAC)</text>
      <line x1="178" y1="76" x2="368" y2="76" stroke="#c0d8a0" stroke-width="0.8"/>
      <text x="273" y="94" text-anchor="middle" font-size="9" fill="#3a6a1a">Linear(376, 512), ReLU</text>
      <text x="273" y="110" text-anchor="middle" font-size="9" fill="#3a6a1a">Linear(512, 512), ReLU</text>
      <text x="273" y="128" text-anchor="middle" font-size="9" fill="#3a6a1a">mu head (512->17)</text>
      <text x="273" y="144" text-anchor="middle" font-size="9" fill="#3a6a1a">logstd head (512->17)</text>
      <text x="273" y="162" text-anchor="middle" font-size="9" fill="#3a6a1a">a = tanh(N(mu, exp(logstd)))</text>
      <rect x="178" y="170" width="195" height="10" fill="#e4f0d8" stroke="#90b880"/>
      <text x="273" y="179" text-anchor="middle" font-size="7.5" fill="#1a4a0a">Auto-alpha: H_target = -17</text>

      <!-- CROSSQ CRITIC -- THE KEY BLOCK -->
      <rect x="168" y="210" width="210" height="230" fill="#eaf0f8" stroke="#3060a0" stroke-width="2"/>
      <text x="273" y="235" text-anchor="middle" font-size="11" font-weight="600" fill="#1a3a6a">CrossQ Critic (x2)</text>
      <text x="273" y="251" text-anchor="middle" font-size="8.5" fill="#aaa">Input: [s, a] in R^393</text>
      <line x1="178" y1="258" x2="368" y2="258" stroke="#b0c8e0" stroke-width="0.8"/>
      <text x="273" y="275" text-anchor="middle" font-size="9" fill="#1a4a7a">Linear(393, 1024)</text>

      <!-- BN 1 highlighted -->
      <rect x="182" y="282" width="182" height="22" fill="#dce8f8" stroke="#3060a0" stroke-width="1.5"/>
      <text x="273" y="297" text-anchor="middle" font-size="9" font-weight="600" fill="#1a3a7a">BatchNorm1d(1024)  [KEY]</text>

      <text x="273" y="320" text-anchor="middle" font-size="9" fill="#1a4a7a">ReLU</text>
      <text x="273" y="336" text-anchor="middle" font-size="9" fill="#1a4a7a">Linear(1024, 1024)</text>

      <!-- BN 2 highlighted -->
      <rect x="182" y="343" width="182" height="22" fill="#dce8f8" stroke="#3060a0" stroke-width="1.5"/>
      <text x="273" y="358" text-anchor="middle" font-size="9" font-weight="600" fill="#1a3a7a">BatchNorm1d(1024)  [KEY]</text>

      <text x="273" y="378" text-anchor="middle" font-size="9" fill="#1a4a7a">ReLU</text>
      <text x="273" y="395" text-anchor="middle" font-size="9" fill="#1a4a7a">Linear(1024, 1) -> Q(s,a)</text>
      <rect x="182" y="405" width="182" height="28" fill="#d0e4f8" stroke="#3060a0"/>
      <text x="273" y="419" text-anchor="middle" font-size="7.5" fill="#1a3a6a">No target networks -- BatchNorm</text>
      <text x="273" y="431" text-anchor="middle" font-size="7.5" fill="#1a3a6a">provides implicit stability</text>

      <!-- UTD BOX -->
      <rect x="428" y="210" width="155" height="90" fill="#f0eaf8" stroke="#7050a0" stroke-width="1.2"/>
      <text x="505" y="233" text-anchor="middle" font-size="10" font-weight="600" fill="#3a1a6a">High UTD Ratio</text>
      <text x="505" y="252" text-anchor="middle" font-size="8.5" fill="#5a3a8a">gradient_steps = 4</text>
      <text x="505" y="268" text-anchor="middle" font-size="8.5" fill="#5a3a8a">train_freq     = 1</text>
      <text x="505" y="285" text-anchor="middle" font-size="8.5" fill="#5a3a8a">4 updates / env step</text>
      <text x="505" y="301" text-anchor="middle" font-size="8" fill="#7a5aaa">safe only with BatchNorm</text>

      <!-- BELLMAN TARGET -->
      <rect x="428" y="320" width="155" height="90" fill="#f0eaf8" stroke="#7050a0" stroke-width="1.2"/>
      <text x="505" y="343" text-anchor="middle" font-size="9.5" font-weight="600" fill="#3a1a6a">Bellman Target</text>
      <text x="505" y="360" text-anchor="middle" font-size="8" fill="#5a3a8a">y = r + gamma *</text>
      <text x="505" y="376" text-anchor="middle" font-size="8" fill="#5a3a8a">(min(Q1(s',a'),Q2(s',a'))</text>
      <text x="505" y="392" text-anchor="middle" font-size="8" fill="#5a3a8a"> - alpha*log pi(a'|s'))</text>
      <text x="505" y="408" text-anchor="middle" font-size="7.5" fill="#3a1a6a">No target net primes (Q1 not Q1')</text>

      <!-- BENEFITS PANEL -->
      <rect x="620" y="30" width="345" height="420" fill="#eaf0ea" stroke="#60a060" stroke-width="1"/>
      <text x="792" y="55" text-anchor="middle" font-size="10" font-weight="600" fill="#1a4a1a">Why CrossQ Outperforms V4</text>
      <line x1="630" y1="62" x2="958" y2="62" stroke="#b0d0b0" stroke-width="0.8"/>

      <text x="632" y="82" font-size="8.5" fill="#1a5a1a">[B1] BatchNorm stabilizes Q-value scale</text>
      <text x="632" y="98" font-size="8" fill="#777">     Q-fn output bounded across training</text>
      <text x="632" y="114" font-size="8" fill="#777">     gradients stable at 10M+ steps</text>

      <text x="632" y="134" font-size="8.5" fill="#1a5a1a">[B2] No target networks</text>
      <text x="632" y="150" font-size="8" fill="#777">     critic sees freshest estimates</text>
      <text x="632" y="166" font-size="8" fill="#777">     faster convergence, less lag</text>

      <text x="632" y="186" font-size="8.5" fill="#1a5a1a">[B3] UTD=4 (was 1)</text>
      <text x="632" y="202" font-size="8" fill="#777">     4x more gradient steps/env step</text>
      <text x="632" y="218" font-size="8" fill="#777">     ~4x sample efficiency gain</text>

      <text x="632" y="238" font-size="8.5" fill="#1a5a1a">[B4] Wider critic [1024,1024]</text>
      <text x="632" y="254" font-size="8" fill="#777">     vs [512,512] in V4</text>
      <text x="632" y="270" font-size="8" fill="#777">     better Q-fn approximation</text>

      <text x="632" y="290" font-size="8.5" fill="#1a5a1a">[B5] Lipschitz regularization (implicit)</text>
      <text x="632" y="306" font-size="8" fill="#777">     BatchNorm bounds Q change rate</text>
      <text x="632" y="322" font-size="8" fill="#777">     provably stable (Bhatt 2024)</text>

      <line x1="630" y1="340" x2="958" y2="340" stroke="#b0d0b0" stroke-width="0.8"/>
      <text x="792" y="360" text-anchor="middle" font-size="8.5" fill="#1a5a1a">Install: pip install sb3-contrib</text>
      <text x="792" y="378" text-anchor="middle" font-size="8.5" fill="#1a5a1a">from sb3_contrib import CrossQ</text>
      <text x="792" y="396" text-anchor="middle" font-size="8.5" fill="#1a5a1a">model = CrossQ("MlpPolicy", env,</text>
      <text x="792" y="412" text-anchor="middle" font-size="8.5" fill="#1a5a1a">  policy_kwargs=dict(</text>
      <text x="792" y="428" text-anchor="middle" font-size="8.5" fill="#1a5a1a">    net_arch=[1024,1024]))</text>
      <text x="792" y="446" text-anchor="middle" font-size="8" fill="#aaa">Expected: 5,500-6,500 @ 1M steps</text>

      <!-- ARROWS -->
      <line x1="130" y1="218" x2="166" y2="120" stroke="#1a5030" stroke-width="1.2" marker-end="url(#a3)"/>
      <line x1="130" y1="232" x2="166" y2="320" stroke="#1a5030" stroke-width="1.2" marker-end="url(#a3)"/>
      <line x1="378" y1="100" x2="618" y2="80" stroke="#1a5030" stroke-width="1" stroke-dasharray="4,2"/>
      <line x1="378" y1="310" x2="426" y2="255" stroke="#1a5030" stroke-width="1.2" marker-end="url(#a3)"/>
      <line x1="378" y1="360" x2="426" y2="365" stroke="#1a5030" stroke-width="1.2" marker-end="url(#a3)"/>
    </svg>
    <div class="diagram-caption">Figure 3. CrossQ Architecture (Bhatt et al., ICLR 2024). BatchNorm layers (highlighted in blue) are the primary innovation. Note absence of target network primes in the Bellman target. Wider critic [1024,1024] vs V4 [512,512].</div>
  </div>
</section>

<!-- ══════════ S6: COMPARISON ══════════ -->
<section id="s6">
  <span class="section-num">Section 6</span>
  <h2>Architectural Comparison</h2>

  <h3>6.1 Performance at 1M Environment Steps</h3>
  <div class="scoreboard">
    <div class="score-row">
      <span class="score-label">V3 TD3 (original)</span>
      <div class="score-track"><div class="score-fill" style="width:4%;background:#c8a0a0;color:#5a1010">300</div></div>
      <span class="score-val bad">~300 reward</span>
    </div>
    <div class="score-row">
      <span class="score-label">V3 TD3 (if stable)</span>
      <div class="score-track"><div class="score-fill" style="width:10%;background:#c8a0a0;color:#5a1010">800</div></div>
      <span class="score-val bad">~800 reward</span>
    </div>
    <div class="score-row">
      <span class="score-label">V4 SAC (fixed)</span>
      <div class="score-track"><div class="score-fill" style="width:42%;background:#c8b870;color:#3a3000">2000-5000</div></div>
      <span class="score-val warn">~3500 reward</span>
    </div>
    <div class="score-row">
      <span class="score-label">CrossQ SAC</span>
      <div class="score-track"><div class="score-fill" style="width:80%;background:#80b890;color:#0a2a18">5500-6500</div></div>
      <span class="score-val good">~6000 reward</span>
    </div>
    <div class="score-row">
      <span class="score-label">REDQ (N=10 Q-nets)</span>
      <div class="score-track"><div class="score-fill" style="width:72%;background:#80b890;color:#0a2a18">5000-6000</div></div>
      <span class="score-val good">~5500 reward</span>
    </div>
    <div class="score-row">
      <span class="score-label">TD-MPC2 (world model)</span>
      <div class="score-track"><div class="score-fill" style="width:93%;background:#9080b8;color:#1a0a3a">6000-8000</div></div>
      <span class="score-val best">~7000 reward</span>
    </div>
  </div>

  <h3>6.2 Feature-by-Feature Comparison</h3>
  <table class="data-table">
    <caption>Table 1. Architectural and hyperparameter comparison across all three configurations evaluated on Humanoid-v4.</caption>
    <tr>
      <th>Component</th>
      <th>V3 (Original)</th>
      <th>V4 (Fixed)</th>
      <th>CrossQ SOTA</th>
    </tr>
    <tr>
      <td class="feature">Algorithm</td>
      <td class="bad">TD3 / DDPG</td>
      <td class="warn">SAC</td>
      <td class="good">CrossQ + SAC</td>
    </tr>
    <tr>
      <td class="feature">Policy type</td>
      <td class="bad">Deterministic mu(s)</td>
      <td class="good">Stochastic N(mu,sigma)</td>
      <td class="good">Stochastic N(mu,sigma)</td>
    </tr>
    <tr>
      <td class="feature">Entropy regularization</td>
      <td class="bad">None</td>
      <td class="good">Auto-alpha, H=-17</td>
      <td class="good">Auto-alpha, H=-17</td>
    </tr>
    <tr>
      <td class="feature">Actor architecture</td>
      <td class="bad">[400, 300], no norm</td>
      <td class="warn">[512, 512], no norm</td>
      <td class="warn">[512, 512], no norm</td>
    </tr>
    <tr>
      <td class="feature">Critic architecture</td>
      <td class="bad">[400, 300], no norm</td>
      <td class="warn">[512, 512], no norm</td>
      <td class="good">[1024, 1024] + BatchNorm</td>
    </tr>
    <tr>
      <td class="feature">Batch Normalization</td>
      <td class="bad">None</td>
      <td class="bad">None</td>
      <td class="good">Between every critic layer</td>
    </tr>
    <tr>
      <td class="feature">Target networks</td>
      <td>Actor + 2x Critics (tau=0.005)</td>
      <td>2x Critics (tau=0.005)</td>
      <td class="good">None (removed)</td>
    </tr>
    <tr>
      <td class="feature">Gradient norm clipping</td>
      <td class="bad">None -- diverges</td>
      <td class="good">max_grad_norm=10</td>
      <td class="good">max_grad_norm=10</td>
    </tr>
    <tr>
      <td class="feature">Adam epsilon</td>
      <td class="bad">1e-8 (NaN risk)</td>
      <td class="good">1e-5</td>
      <td class="good">1e-5</td>
    </tr>
    <tr>
      <td class="feature">Update-to-data ratio</td>
      <td class="bad">1 (gradient_steps=1)</td>
      <td class="warn">1 (higher diverges)</td>
      <td class="good">4 (safe with BatchNorm)</td>
    </tr>
    <tr>
      <td class="feature">Replay buffer size</td>
      <td class="bad">300k (= timesteps, OOM)</td>
      <td class="good">500k cap</td>
      <td class="good">500k cap</td>
    </tr>
    <tr>
      <td class="feature">Memory optimization</td>
      <td class="bad">None (~5-7 GB)</td>
      <td class="good">optimize_memory=True (~1.7 GB)</td>
      <td class="good">optimize_memory=True (~1.7 GB)</td>
    </tr>
    <tr>
      <td class="feature">handle_timeout_termination</td>
      <td>True (default, no conflict)</td>
      <td class="good">False (required for opt. memory)</td>
      <td class="good">False</td>
    </tr>
    <tr>
      <td class="feature">clip_obs (VecNormalize)</td>
      <td class="bad">10.0 (too loose)</td>
      <td class="good">5.0</td>
      <td class="good">5.0</td>
    </tr>
    <tr>
      <td class="feature">clip_reward (VecNormalize)</td>
      <td class="bad">None</td>
      <td class="good">10.0</td>
      <td class="good">10.0</td>
    </tr>
    <tr>
      <td class="feature">Stable at 1M steps</td>
      <td class="bad">No (crashes)</td>
      <td class="good">Yes</td>
      <td class="good">Yes</td>
    </tr>
    <tr>
      <td class="feature">Expected reward @ 1M</td>
      <td class="bad">300-800</td>
      <td class="warn">2,000-5,000</td>
      <td class="good">5,500-6,500</td>
    </tr>
    <tr>
      <td class="feature">Implementation effort</td>
      <td>Baseline</td>
      <td class="good">Provided (V4 notebook)</td>
      <td class="good">pip install sb3-contrib + 3 lines</td>
    </tr>
  </table>

  <h3>6.3 Parameter Count Comparison</h3>
  <div class="param-grid">
    <div class="param-block">
      <h5>V3 TD3 -- Parameter Budget</h5>
      <div class="param-row"><span class="param-key">Actor L1</span><span class="param-val">376*400 + 400 = 150,800</span></div>
      <div class="param-row"><span class="param-key">Actor L2</span><span class="param-val">400*300 + 300 = 120,300</span></div>
      <div class="param-row"><span class="param-key">Actor L3</span><span class="param-val">300*17  + 17  = 5,117</span></div>
      <div class="param-row"><span class="param-key">Actor total</span><span class="param-val">276,217</span></div>
      <div class="param-row"><span class="param-key">Critic x2</span><span class="param-val">393*400+300*300+300*1 = 248,500 x2</span></div>
      <div class="param-row"><span class="param-key">Total params</span><span class="param-val">~773,000</span></div>
    </div>
    <div class="param-block">
      <h5>CrossQ SAC -- Parameter Budget</h5>
      <div class="param-row"><span class="param-key">Actor L1</span><span class="param-val">376*512 + 512 = 193,024</span></div>
      <div class="param-row"><span class="param-key">Actor L2</span><span class="param-val">512*512 + 512 = 262,656</span></div>
      <div class="param-row"><span class="param-key">Actor L3+L4</span><span class="param-val">2*(512*17+17) = 17,442</span></div>
      <div class="param-row"><span class="param-key">Critic L1</span><span class="param-val">393*1024 + 1024 = 403,456</span></div>
      <div class="param-row"><span class="param-key">Critic L2</span><span class="param-val">1024*1024+1024 = 1,049,600</span></div>
      <div class="param-row"><span class="param-key">BN params (x2)</span><span class="param-val">2*(1024*2) = 4,096 each</span></div>
      <div class="param-row"><span class="param-key">Total params</span><span class="param-val">~3,200,000</span></div>
    </div>
  </div>
</section>

<!-- ══════════ S7: UPGRADE ROADMAP ══════════ -->
<section id="s7">
  <span class="section-num">Section 7</span>
  <h2>Upgrade Roadmap</h2>

  <p>
    The following table ranks architectural upgrades by implementation effort versus expected
    reward gain on Humanoid-v4 at 1M environment steps. All estimates assume training from
    scratch with a fixed compute budget (1M steps, single GPU or CPU).
  </p>

  <table class="data-table">
    <caption>Table 2. Ranked upgrade path from V4 baseline. Cumulative reward assumes each upgrade is applied on top of the previous.</caption>
    <tr>
      <th>Priority</th>
      <th>Upgrade</th>
      <th>Expected Gain</th>
      <th>Effort</th>
      <th>Key Change</th>
      <th>Reference</th>
    </tr>
    <tr>
      <td class="feature">1 (done)</td>
      <td>TD3 to SAC</td>
      <td class="warn">+1,500-2,500</td>
      <td>Config change</td>
      <td>Stochastic policy, entropy bonus, auto-alpha</td>
      <td>Haarnoja 2018</td>
    </tr>
    <tr>
      <td class="feature">2</td>
      <td>CrossQ critic</td>
      <td class="good">+2,000-3,000</td>
      <td>3 lines of code</td>
      <td>BatchNorm in critic, remove target nets, UTD=4</td>
      <td>Bhatt 2024</td>
    </tr>
    <tr>
      <td class="feature">3</td>
      <td>Wider critic [1024,1024]</td>
      <td class="warn">+300-600</td>
      <td>1 line (net_arch)</td>
      <td>Increased critic capacity for Q-function</td>
      <td>Bhatt 2024</td>
    </tr>
    <tr>
      <td class="feature">4</td>
      <td>REDQ (N=10 critics)</td>
      <td class="good">+1,000-2,000</td>
      <td>New library</td>
      <td>Ensemble of 10 Q-nets, UTD=20, randomized target</td>
      <td>Chen 2021</td>
    </tr>
    <tr>
      <td class="feature">5</td>
      <td>Longer training (3M steps)</td>
      <td class="good">+1,000-2,000</td>
      <td>Config change</td>
      <td>More environment interactions, same architecture</td>
      <td>--</td>
    </tr>
    <tr>
      <td class="feature">6</td>
      <td>TD-MPC2 (world model)</td>
      <td class="best">+2,000-4,000</td>
      <td>Full reimplementation</td>
      <td>Latent dynamics model, model-predictive control</td>
      <td>Hansen 2024</td>
    </tr>
    <tr>
      <td class="feature">7</td>
      <td>DreamerV3</td>
      <td class="best">+3,000-5,000</td>
      <td>Full reimplementation</td>
      <td>RSSM world model, actor-critic in latent space</td>
      <td>Hafner 2023</td>
    </tr>
  </table>

  <h3>7.1 Minimum Code Change to CrossQ</h3>
  <pre>pip install sb3-contrib

# In Cell 3 of notebook, add:
from sb3_contrib import CrossQ

# Replace model initialization in Cell 10:
model = CrossQ(
    "MlpPolicy",
    train_env,
    learning_rate=1e-4,               # CrossQ prefers lower LR than SAC
    buffer_size=500_000,
    optimize_memory_usage=True,
    replay_buffer_kwargs=dict(handle_timeout_termination=False),
    batch_size=256,
    learning_starts=10_000,
    gradient_steps=4,                 # high UTD, safe due to BatchNorm
    train_freq=1,
    policy_kwargs=dict(
        net_arch=[1024, 1024],
        activation_fn=torch.nn.ReLU,
        optimizer_kwargs=dict(eps=1e-5),
    ),
    seed=42,
    device=DEVICE,
)
# No other changes required. Same callback, env, and video recording.</pre>

  <div class="callout good">
    <div class="callout-label">Expected Outcome</div>
    <p>CrossQ with the above settings should reach 5,000-6,500 reward at 1M steps on
    Humanoid-v4, compared to 2,000-5,000 for V4 SAC. The primary gains come from
    BatchNorm stability enabling UTD=4 (4x sample efficiency) and the absence of
    target network lag. Training time per step will be approximately 4x slower due to
    the higher gradient step count, but wall-clock time to a given reward threshold
    is lower overall.</p>
  </div>
</section>

<!-- ══════════ REFERENCES ══════════ -->
<section id="s8" class="references">
  <span class="section-num">Section 8</span>
  <h2>References</h2>
  <ol>
    <li>Bhatt, A., Palenicek, D., Belousov, B., Argus, M., Amiranashvili, A., Brox, T., Peters, J. (2024). <em>CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity.</em> ICLR 2024. <span>arXiv:2301.02328</span></li>
    <li>Haarnoja, T., Zhou, A., Abbeel, P., Levine, S. (2018). <em>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.</em> ICML 2018. <span>arXiv:1801.01290</span></li>
    <li>Fujimoto, S., van Hoof, H., Meger, D. (2018). <em>Addressing Function Approximation Error in Actor-Critic Methods.</em> ICML 2018. <span>arXiv:1802.09477</span></li>
    <li>Chen, X., Wang, C., Zhou, Z., Ross, K. (2021). <em>Randomized Ensembled Double Q-Learning: Learning Fast Without a Model.</em> ICLR 2021. <span>arXiv:2101.05982</span></li>
    <li>Hansen, N., Su, H., Wang, X. (2024). <em>TD-MPC2: Scalable, Robust World Models for Continuous Control.</em> ICLR 2024. <span>arXiv:2310.16828</span></li>
    <li>Hafner, D., Lillicrap, T., Norouzi, M., Ba, J. (2023). <em>Mastering Diverse Domains through World Models.</em> <span>arXiv:2301.04104</span></li>
    <li>Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D. (2016). <em>Continuous control with deep reinforcement learning.</em> ICLR 2016. <span>arXiv:1509.02971</span></li>
    <li>Ioffe, S., Szegedy, C. (2015). <em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.</em> ICML 2015. <span>arXiv:1502.03167</span></li>
    <li>Todorov, E., Erez, T., Tassa, Y. (2012). <em>MuJoCo: A physics engine for model-based control.</em> IROS 2012.</li>
    <li>Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann, N. (2021). <em>Stable-Baselines3: Reliable Reinforcement Learning Implementations.</em> JMLR. <span>arXiv:2005.05719</span></li>
  </ol>
</section>

</div><!-- /page -->
</body>
</html>
